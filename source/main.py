import os
import requests
import urllib.parse
import urllib3
from github import Github, Auth
from github import GithubException
from datetime import datetime
import zoneinfo
import concurrent.futures
import threading
import re
from collections import defaultdict
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

# -------------------- –õ–û–ì–ò–†–û–í–ê–ù–ò–ï --------------------
LOGS_BY_FILE: dict[int, list[str]] = defaultdict(list)
_LOG_LOCK = threading.Lock()
_UPDATED_FILES_LOCK = threading.Lock()

_GITHUBMIRROR_INDEX_RE = re.compile(r"githubmirror/(\d+)\.txt")
updated_files = set()

def _extract_index(msg: str) -> int:
    """–ü—ã—Ç–∞–µ—Ç—Å—è –∏–∑–≤–ª–µ—á—å –Ω–æ–º–µ—Ä —Ñ–∞–π–ª–∞ –∏–∑ —Å—Ç—Ä–æ–∫–∏ –≤–∏–¥–∞ 'githubmirror/12.txt'."""
    m = _GITHUBMIRROR_INDEX_RE.search(msg)
    if m:
        try:
            return int(m.group(1))
        except ValueError:
            pass
    return 0

def log(message: str):
    """–î–æ–±–∞–≤–ª—è–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏–µ –≤ –æ–±—â–∏–π —Å–ª–æ–≤–∞—Ä—å –ª–æ–≥–æ–≤ –ø–æ—Ç–æ–∫–æ–±–µ–∑–æ–ø–∞—Å–Ω–æ."""
    idx = _extract_index(message)
    with _LOG_LOCK:
        LOGS_BY_FILE[idx].append(message)

# –ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–µ–∫—É—â–µ–≥–æ –≤—Ä–µ–º–µ–Ω–∏ –ø–æ —á–∞—Å–æ–≤–æ–º—É –ø–æ—è—Å—É –ï–≤—Ä–æ–ø–∞/–ú–æ—Å–∫–≤–∞
zone = zoneinfo.ZoneInfo("Europe/Moscow")
thistime = datetime.now(zone)
offset = thistime.strftime("%H:%M | %d.%m.%Y")

# –ü–æ–ª—É—á–µ–Ω–∏–µ GitHub —Ç–æ–∫–µ–Ω–∞ –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
GITHUB_TOKEN = os.environ.get("MY_TOKEN")
REPO_NAME = "AvenCores/goida-vpn-configs"

if GITHUB_TOKEN:
    g = Github(auth=Auth.Token(GITHUB_TOKEN))
else:
    g = Github()

REPO = g.get_repo(REPO_NAME)

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –ª–∏–º–∏—Ç–æ–≤ GitHub API
try:
    remaining, limit = g.rate_limiting
    if remaining < 100:
        log(f"‚ö†Ô∏è –í–Ω–∏–º–∞–Ω–∏–µ: –æ—Å—Ç–∞–ª–æ—Å—å {remaining}/{limit} –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ GitHub API")
    else:
        log(f"‚ÑπÔ∏è –î–æ—Å—Ç—É–ø–Ω–æ –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ GitHub API: {remaining}/{limit}")
except Exception as e:
    log(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –ª–∏–º–∏—Ç—ã GitHub API: {e}")

if not os.path.exists("githubmirror"):
    os.mkdir("githubmirror")

URLS = [
    "https://github.com/sakha1370/OpenRay/raw/refs/heads/main/output/all_valid_proxies.txt", #1
    "https://raw.githubusercontent.com/sevcator/5ubscrpt10n/main/protocols/vl.txt", #2
    "https://raw.githubusercontent.com/yitong2333/proxy-minging/refs/heads/main/v2ray.txt", #3
    "https://raw.githubusercontent.com/acymz/AutoVPN/refs/heads/main/data/V2.txt", #4
    "https://raw.githubusercontent.com/miladtahanian/V2RayCFGDumper/refs/heads/main/config.txt", #5
    "https://raw.githubusercontent.com/roosterkid/openproxylist/main/V2RAY_RAW.txt", #6
    "https://github.com/Epodonios/v2ray-configs/raw/main/Splitted-By-Protocol/trojan.txt", #7
    "https://raw.githubusercontent.com/YasserDivaR/pr0xy/refs/heads/main/ShadowSocks2021.txt", #8
    "https://raw.githubusercontent.com/mohamadfg-dev/telegram-v2ray-configs-collector/refs/heads/main/category/vless.txt", #9
    "https://raw.githubusercontent.com/mheidari98/.proxy/refs/heads/main/vless", #10
    "https://raw.githubusercontent.com/youfoundamin/V2rayCollector/main/mixed_iran.txt", #11
    "https://raw.githubusercontent.com/mheidari98/.proxy/refs/heads/main/all", #12
    "https://github.com/Kwinshadow/TelegramV2rayCollector/raw/refs/heads/main/sublinks/mix.txt", #13
    "https://github.com/LalatinaHub/Mineral/raw/refs/heads/master/result/nodes", #14
    "https://raw.githubusercontent.com/miladtahanian/multi-proxy-config-fetcher/refs/heads/main/configs/proxy_configs.txt", #15
    "https://raw.githubusercontent.com/Pawdroid/Free-servers/refs/heads/main/sub", #16
    "https://github.com/MhdiTaheri/V2rayCollector_Py/raw/refs/heads/main/sub/Mix/mix.txt", #17
    "https://github.com/Epodonios/v2ray-configs/raw/main/Splitted-By-Protocol/vmess.txt", #18
    "https://github.com/MhdiTaheri/V2rayCollector/raw/refs/heads/main/sub/mix", #19
    "https://github.com/Argh94/Proxy-List/raw/refs/heads/main/All_Config.txt", #20
    "https://raw.githubusercontent.com/shabane/kamaji/master/hub/merged.txt", #21
    "https://raw.githubusercontent.com/wuqb2i4f/xray-config-toolkit/main/output/base64/mix-uri", #22
    "https://raw.githubusercontent.com/AzadNetCH/Clash/refs/heads/main/AzadNet.txt", #23
    "https://raw.githubusercontent.com/STR97/STRUGOV/refs/heads/main/STR.BYPASS#STR.BYPASS%F0%9F%91%BE", #24
    "https://raw.githubusercontent.com/V2RayRoot/V2RayConfig/refs/heads/main/Config/vless.txt", #25
]

REMOTE_PATHS = [f"githubmirror/{i+1}.txt" for i in range(len(URLS))]
LOCAL_PATHS = [f"githubmirror/{i+1}.txt" for i in range(len(URLS))]

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

CHROME_UA = (
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
    "AppleWebKit/537.36 (KHTML, like Gecko) "
    "Chrome/138.0.0.0 Safari/537.36"
)

DEFAULT_MAX_WORKERS = int(os.environ.get("MAX_WORKERS", "16"))

def _build_session(max_pool_size: int) -> requests.Session:
    session = requests.Session()
    adapter = HTTPAdapter(
        pool_connections=max_pool_size,
        pool_maxsize=max_pool_size,
        max_retries=Retry(
            total=1,
            backoff_factor=0.2,
            status_forcelist=(429, 500, 502, 503, 504),
            allowed_methods=("HEAD", "GET", "OPTIONS"),
        ),
    )
    session.mount("http://", adapter)
    session.mount("https://", adapter)
    session.headers.update({"User-Agent": CHROME_UA})
    return session

REQUESTS_SESSION = _build_session(max_pool_size=max(DEFAULT_MAX_WORKERS, len(URLS))) if 'URLS' in globals() else _build_session(DEFAULT_MAX_WORKERS)

def fetch_data(url: str, timeout: int = 10, max_attempts: int = 3, session: requests.Session | None = None) -> str:
    sess = session or REQUESTS_SESSION
    for attempt in range(1, max_attempts + 1):
        try:
            modified_url = url
            verify = True

            if attempt == 2:
                verify = False
            elif attempt == 3:
                parsed = urllib.parse.urlparse(url)
                if parsed.scheme == "https":
                    modified_url = parsed._replace(scheme="http").geturl()
                verify = False

            response = sess.get(modified_url, timeout=timeout, verify=verify)
            response.raise_for_status()
            return response.text

        except requests.exceptions.RequestException as exc:
            last_exc = exc
            if attempt < max_attempts:
                continue
            raise last_exc

def save_to_local_file(path, content):
    with open(path, "w", encoding="utf-8") as file:
        file.write(content)
    log(f"üìÅ –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –ª–æ–∫–∞–ª—å–Ω–æ –≤ {path}")

def extract_source_name(url: str) -> str:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø–æ–Ω—è—Ç–Ω–æ–µ –∏–º—è –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –∏–∑ URL"""
    try:
        parsed = urllib.parse.urlparse(url)
        path_parts = parsed.path.split('/')
        if len(path_parts) > 2:
            return f"{path_parts[1]}/{path_parts[2]}"
        return parsed.netloc
    except:
        return "–ò—Å—Ç–æ—á–Ω–∏–∫"

def update_readme_table():
    """–û–±–Ω–æ–≤–ª—è–µ—Ç —Ç–∞–±–ª–∏—Ü—É –≤ README.md —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –≤—Ä–µ–º–µ–Ω–∏ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è"""
    try:
        # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—É—â–∏–π README.md
        try:
            readme_file = REPO.get_contents("README.md")
            old_content = readme_file.decoded_content.decode("utf-8")
        except GithubException as e:
            if e.status == 404:
                log("‚ùå README.md –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏")
                return
            else:
                log(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ README.md: {e}")
                return

        # –†–∞–∑–¥–µ–ª—è–µ–º –≤—Ä–µ–º—è –∏ –¥–∞—Ç—É
        time_part, date_part = offset.split(" | ")
        
        # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é —Ç–∞–±–ª–∏—Ü—É
        table_header = "| ‚Ññ | –§–∞–π–ª | –ò—Å—Ç–æ—á–Ω–∏–∫ | –í—Ä–µ–º—è | –î–∞—Ç–∞ |\n|--|--|--|--|--|"
        table_rows = []
        
        for i, (remote_path, url) in enumerate(zip(REMOTE_PATHS, URLS), 1):
            filename = f"{i}.txt"
            source_name = extract_source_name(url)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –±—ã–ª –ª–∏ —Ñ–∞–π–ª –æ–±–Ω–æ–≤–ª–µ–Ω –≤ —ç—Ç–æ–º –∑–∞–ø—É—Å–∫–µ
            if i in updated_files:
                update_time = time_part
                update_date = date_part
            else:
                # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –≤—Ä–µ–º—è –∏ –¥–∞—Ç—É –∏–∑ —Å—Ç–∞—Ä–æ–π —Ç–∞–±–ª–∏—Ü—ã
                pattern = rf"\|\s*{i}\s*\|\s*`{filename}`.*?\|\s*.*?\|\s*(.*?)\s*\|\s*(.*?)\s*\|"
                match = re.search(pattern, old_content)
                if match:
                    update_time = match.group(1) if match.group(1).strip() else "–ù–∏–∫–æ–≥–¥–∞"
                    update_date = match.group(2) if match.group(2).strip() else "–ù–∏–∫–æ–≥–¥–∞"
                else:
                    update_time = "–ù–∏–∫–æ–≥–¥–∞"
                    update_date = "–ù–∏–∫–æ–≥–¥–∞"
            
            table_rows.append(f"| {i} | `{filename}` | [{source_name}]({url}) | {update_time} | {update_date} |")

        new_table = table_header + "\n" + "\n".join(table_rows)

        # –ó–∞–º–µ–Ω—è–µ–º —Ç–∞–±–ª–∏—Ü—É –≤ README.md
        table_pattern = r"\| ‚Ññ \| –§–∞–π–ª \| –ò—Å—Ç–æ—á–Ω–∏–∫ \| –í—Ä–µ–º—è \| –î–∞—Ç–∞ \|[\s\S]*?\|--\|--\|--\|--\|--\|[\s\S]*?(\n\n## |$)"
        new_content = re.sub(table_pattern, new_table + r"\1", old_content)

        if new_content != old_content:
            REPO.update_file(
                path="README.md",
                message="üìù –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã –≤ README.md",
                content=new_content,
                sha=readme_file.sha
            )
            log("üìù –¢–∞–±–ª–∏—Ü–∞ –≤ README.md –æ–±–Ω–æ–≤–ª–µ–Ω–∞")
        else:
            log("üìù –¢–∞–±–ª–∏—Ü–∞ –≤ README.md –Ω–µ —Ç—Ä–µ–±—É–µ—Ç –∏–∑–º–µ–Ω–µ–Ω–∏–π")

    except Exception as e:
        log(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ README.md: {e}")

def upload_to_github(local_path, remote_path):
    if not os.path.exists(local_path):
        log(f"‚ùå –§–∞–π–ª {local_path} –Ω–µ –Ω–∞–π–¥–µ–Ω.")
        return

    repo = REPO

    with open(local_path, "r", encoding="utf-8") as file:
        content = file.read()

    max_retries = 5
    import time

    for attempt in range(1, max_retries + 1):
        try:
            try:
                file_in_repo = repo.get_contents(remote_path)
                current_sha = file_in_repo.sha
            except GithubException as e_get:
                if getattr(e_get, "status", None) == 404:
                    basename = os.path.basename(remote_path)
                    repo.create_file(
                        path=remote_path,
                        message=f"üÜï –ü–µ—Ä–≤—ã–π –∫–æ–º–º–∏—Ç {basename} –ø–æ —á–∞—Å–æ–≤–æ–º—É –ø–æ—è—Å—É –ï–≤—Ä–æ–ø–∞/–ú–æ—Å–∫–≤–∞: {offset}",
                        content=content,
                    )
                    log(f"üÜï –§–∞–π–ª {remote_path} —Å–æ–∑–¥–∞–Ω.")
                    # –î–æ–±–∞–≤–ª—è–µ–º –≤ –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã
                    file_index = int(remote_path.split('/')[1].split('.')[0])
                    with _UPDATED_FILES_LOCK:
                        updated_files.add(file_index)
                    return
                else:
                    msg = e_get.data.get("message", str(e_get))
                    log(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ {remote_path}: {msg}")
                    return

            try:
                remote_content = file_in_repo.decoded_content.decode("utf-8", errors="replace")
                if remote_content == content:
                    log(f"üîÑ –ò–∑–º–µ–Ω–µ–Ω–∏–π –¥–ª—è {remote_path} –Ω–µ—Ç.")
                    return
            except Exception:
                pass

            basename = os.path.basename(remote_path)
            try:
                repo.update_file(
                    path=remote_path,
                    message=f"üöÄ –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ {basename} –ø–æ —á–∞—Å–æ–≤–æ–º—É –ø–æ—è—Å—É –ï–≤—Ä–æ–ø–∞/–ú–æ—Å–∫–≤–∞: {offset}",
                    content=content,
                    sha=current_sha,
                )
                log(f"üöÄ –§–∞–π–ª {remote_path} –æ–±–Ω–æ–≤–ª—ë–Ω –≤ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏.")
                # –î–æ–±–∞–≤–ª—è–µ–º –≤ –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã
                file_index = int(remote_path.split('/')[1].split('.')[0])
                with _UPDATED_FILES_LOCK:
                    updated_files.add(file_index)
                return
            except GithubException as e_upd:
                if getattr(e_upd, "status", None) == 409:
                    if attempt < max_retries:
                        wait_time = 0.5 * (2 ** (attempt - 1))
                        log(f"‚ö†Ô∏è –ö–æ–Ω—Ñ–ª–∏–∫—Ç SHA –¥–ª—è {remote_path}, –ø–æ–ø—ã—Ç–∫–∞ {attempt}/{max_retries}, –∂–¥–µ–º {wait_time} —Å–µ–∫")
                        time.sleep(wait_time)
                        continue
                    else:
                        log(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –æ–±–Ω–æ–≤–∏—Ç—å {remote_path} –ø–æ—Å–ª–µ {max_retries} –ø–æ–ø—ã—Ç–æ–∫")
                        return
                else:
                    msg = e_upd.data.get("message", str(e_upd))
                    log(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {remote_path}: {msg}")
                    return

        except Exception as e_general:
            short_msg = str(e_general)
            if len(short_msg) > 200:
                short_msg = short_msg[:200] + "‚Ä¶"
            log(f"‚ö†Ô∏è –ù–µ–ø—Ä–µ–¥–≤–∏–¥–µ–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ {remote_path}: {short_msg}")
            return

    log(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –æ–±–Ω–æ–≤–∏—Ç—å {remote_path} –ø–æ—Å–ª–µ {max_retries} –ø–æ–ø—ã—Ç–æ–∫")

def download_and_save(idx):
    url = URLS[idx]
    local_path = LOCAL_PATHS[idx]
    try:
        data = fetch_data(url)

        if os.path.exists(local_path):
            try:
                with open(local_path, "r", encoding="utf-8") as f_old:
                    old_data = f_old.read()
                if old_data == data:
                    log(f"üîÑ –ò–∑–º–µ–Ω–µ–Ω–∏–π –¥–ª—è {local_path} –Ω–µ—Ç (–ª–æ–∫–∞–ª—å–Ω–æ). –ü—Ä–æ–ø—É—Å–∫ –∑–∞–≥—Ä—É–∑–∫–∏ –≤ GitHub.")
                    return None
            except Exception:
                pass

        save_to_local_file(local_path, data)
        return local_path, REMOTE_PATHS[idx]
    except Exception as e:
        short_msg = str(e)
        if len(short_msg) > 200:
            short_msg = short_msg[:200] + "‚Ä¶"
        log(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–∏ {url}: {short_msg}")
        return None

def main(dry_run: bool = False):
    max_workers_download = min(DEFAULT_MAX_WORKERS, max(1, len(URLS)))
    max_workers_upload = max(2, min(6, len(URLS)))

    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers_download) as download_pool, \
         concurrent.futures.ThreadPoolExecutor(max_workers=max_workers_upload) as upload_pool:

        download_futures = [download_pool.submit(download_and_save, i) for i in range(len(URLS))]
        upload_futures: list[concurrent.futures.Future] = []

        for future in concurrent.futures.as_completed(download_futures):
            result = future.result()
            if result:
                local_path, remote_path = result
                if dry_run:
                    log(f"‚ÑπÔ∏è Dry-run: –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –∑–∞–≥—Ä—É–∑–∫—É {remote_path} (–ª–æ–∫–∞–ª—å–Ω—ã–π –ø—É—Ç—å {local_path})")
                else:
                    upload_futures.append(upload_pool.submit(upload_to_github, local_path, remote_path))

        for uf in concurrent.futures.as_completed(upload_futures):
            _ = uf.result()

    # –û–±–Ω–æ–≤–ª—è–µ–º —Ç–∞–±–ª–∏—Ü—É –≤ README.md –ø–æ—Å–ª–µ –≤—Å–µ—Ö –∑–∞–≥—Ä—É–∑–æ–∫
    if not dry_run and updated_files:
        update_readme_table()

    # –í—ã–≤–æ–¥ –ª–æ–≥–æ–≤
    ordered_keys = sorted(k for k in LOGS_BY_FILE.keys() if k != 0)
    output_lines: list[str] = []

    for k in ordered_keys:
        output_lines.append(f"----- {k}.txt -----")
        output_lines.extend(LOGS_BY_FILE[k])

    if LOGS_BY_FILE.get(0):
        output_lines.append("----- –û–±—â–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è -----")
        output_lines.extend(LOGS_BY_FILE[0])

    print("\n".join(output_lines))

if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="–°–∫–∞—á–∏–≤–∞–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥–æ–≤ –∏ –∑–∞–≥—Ä—É–∑–∫–∞ –≤ GitHub")
    parser.add_argument("--dry-run", action="store_true", help="–¢–æ–ª—å–∫–æ —Å–∫–∞—á–∏–≤–∞—Ç—å –∏ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å –ª–æ–∫–∞–ª—å–Ω–æ, –Ω–µ –∑–∞–≥—Ä—É–∂–∞—Ç—å –≤ GitHub")
    args = parser.parse_args()

    main(dry_run=args.dry_run)