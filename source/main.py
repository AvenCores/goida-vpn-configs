import os
import requests
import urllib.parse
import urllib3
from github import Github, Auth
from github import GithubException
from datetime import datetime
import zoneinfo
import concurrent.futures
import threading
import re
import json
import base64
from collections import defaultdict
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

# -------------------- –õ–û–ì–ò–†–û–í–ê–ù–ò–ï --------------------
LOGS_BY_FILE: dict[int, list[str]] = defaultdict(list)
_LOG_LOCK = threading.Lock()
_UPDATED_FILES_LOCK = threading.Lock()

_GITHUBMIRROR_INDEX_RE = re.compile(r"githubmirror/(\d+)\.txt")
updated_files = set()

def _extract_index(msg: str) -> int:
    """–ü—ã—Ç–∞–µ—Ç—Å—è –∏–∑–≤–ª–µ—á—å –Ω–æ–º–µ—Ä —Ñ–∞–π–ª–∞ –∏–∑ —Å—Ç—Ä–æ–∫–∏ –≤–∏–¥–∞ 'githubmirror/12.txt'."""
    m = _GITHUBMIRROR_INDEX_RE.search(msg)
    if m:
        try:
            return int(m.group(1))
        except ValueError:
            pass
    return 0

def log(message: str):
    """–î–æ–±–∞–≤–ª—è–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏–µ –≤ –æ–±—â–∏–π —Å–ª–æ–≤–∞—Ä—å –ª–æ–≥–æ–≤ –ø–æ—Ç–æ–∫–æ–±–µ–∑–æ–ø–∞—Å–Ω–æ."""
    idx = _extract_index(message)
    with _LOG_LOCK:
        LOGS_BY_FILE[idx].append(message)

# –ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–µ–∫—É—â–µ–≥–æ –≤—Ä–µ–º–µ–Ω–∏ –ø–æ —á–∞—Å–æ–≤–æ–º—É –ø–æ—è—Å—É –ï–≤—Ä–æ–ø–∞/–ú–æ—Å–∫–≤–∞
zone = zoneinfo.ZoneInfo("Europe/Moscow")
thistime = datetime.now(zone)
offset = thistime.strftime("%H:%M | %d.%m.%Y")

# –ü–æ–ª—É—á–µ–Ω–∏–µ GitHub —Ç–æ–∫–µ–Ω–∞ –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
GITHUB_TOKEN = os.environ.get("MY_TOKEN")
REPO_NAME = "AvenCores/goida-vpn-configs"

if GITHUB_TOKEN:
    g = Github(auth=Auth.Token(GITHUB_TOKEN))
else:
    g = Github()

REPO = g.get_repo(REPO_NAME)

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –ª–∏–º–∏—Ç–æ–≤ GitHub API
try:
    remaining, limit = g.rate_limiting
    if remaining < 100:
        log(f"‚ö†Ô∏è –í–Ω–∏–º–∞–Ω–∏–µ: –æ—Å—Ç–∞–ª–æ—Å—å {remaining}/{limit} –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ GitHub API")
    else:
        log(f"‚ÑπÔ∏è –î–æ—Å—Ç—É–ø–Ω–æ –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ GitHub API: {remaining}/{limit}")
except Exception as e:
    log(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –ª–∏–º–∏—Ç—ã GitHub API: {e}")

if not os.path.exists("githubmirror"):
    os.mkdir("githubmirror")

URLS = [
    "https://github.com/sakha1370/OpenRay/raw/refs/heads/main/output/all_valid_proxies.txt", #1
    "https://raw.githubusercontent.com/sevcator/5ubscrpt10n/main/protocols/vl.txt", #2
    "https://raw.githubusercontent.com/yitong2333/proxy-minging/refs/heads/main/v2ray.txt", #3
    "https://raw.githubusercontent.com/acymz/AutoVPN/refs/heads/main/data/V2.txt", #4
    "https://raw.githubusercontent.com/miladtahanian/V2RayCFGDumper/refs/heads/main/config.txt", #5
    "https://raw.githubusercontent.com/roosterkid/openproxylist/main/V2RAY_RAW.txt", #6
    "https://github.com/Epodonios/v2ray-configs/raw/main/Splitted-By-Protocol/trojan.txt", #7
    "https://raw.githubusercontent.com/YasserDivaR/pr0xy/refs/heads/main/ShadowSocks2021.txt", #8
    "https://raw.githubusercontent.com/mohamadfg-dev/telegram-v2ray-configs-collector/refs/heads/main/category/vless.txt", #9
    "https://raw.githubusercontent.com/mheidari98/.proxy/refs/heads/main/vless", #10
    "https://raw.githubusercontent.com/youfoundamin/V2rayCollector/main/mixed_iran.txt", #11
    "https://raw.githubusercontent.com/mheidari98/.proxy/refs/heads/main/all", #12
    "https://github.com/Kwinshadow/TelegramV2rayCollector/raw/refs/heads/main/sublinks/mix.txt", #13
    "https://github.com/LalatinaHub/Mineral/raw/refs/heads/master/result/nodes", #14
    "https://raw.githubusercontent.com/miladtahanian/multi-proxy-config-fetcher/refs/heads/main/configs/proxy_configs.txt", #15
    "https://raw.githubusercontent.com/Pawdroid/Free-servers/refs/heads/main/sub", #16
    "https://github.com/MhdiTaheri/V2rayCollector_Py/raw/refs/heads/main/sub/Mix/mix.txt", #17
    "https://github.com/Epodonios/v2ray-configs/raw/main/Splitted-By-Protocol/vmess.txt", #18
    "https://github.com/MhdiTaheri/V2rayCollector/raw/refs/heads/main/sub/mix", #19
    "https://github.com/Argh94/Proxy-List/raw/refs/heads/main/All_Config.txt", #20
    "https://raw.githubusercontent.com/shabane/kamaji/master/hub/merged.txt", #21
    "https://raw.githubusercontent.com/wuqb2i4f/xray-config-toolkit/main/output/base64/mix-uri", #22
    "https://raw.githubusercontent.com/AzadNetCH/Clash/refs/heads/main/AzadNet.txt", #23
    "https://raw.githubusercontent.com/STR97/STRUGOV/refs/heads/main/STR.BYPASS#STR.BYPASS%F0%9F%91%BE", #24
    "https://raw.githubusercontent.com/V2RayRoot/V2RayConfig/refs/heads/main/Config/vless.txt", #25
]

REMOTE_PATHS = [f"githubmirror/{i+1}.txt" for i in range(len(URLS))]
LOCAL_PATHS = [f"githubmirror/{i+1}.txt" for i in range(len(URLS))]

# –î–æ–±–∞–≤–ª—è–µ–º 26-–π —Ñ–∞–π–ª –≤ –ø—É—Ç–∏
REMOTE_PATHS.append("githubmirror/26.txt")
LOCAL_PATHS.append("githubmirror/26.txt")

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

CHROME_UA = (
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
    "AppleWebKit/537.36 (KHTML, like Gecko) "
    "Chrome/138.0.0.0 Safari/537.36"
)

DEFAULT_MAX_WORKERS = int(os.environ.get("MAX_WORKERS", "16"))

def _build_session(max_pool_size: int) -> requests.Session:
    session = requests.Session()
    adapter = HTTPAdapter(
        pool_connections=max_pool_size,
        pool_maxsize=max_pool_size,
        max_retries=Retry(
            total=1,
            backoff_factor=0.2,
            status_forcelist=(429, 500, 502, 503, 504),
            allowed_methods=("HEAD", "GET", "OPTIONS"),
        ),
    )
    session.mount("http://", adapter)
    session.mount("https://", adapter)
    session.headers.update({"User-Agent": CHROME_UA})
    return session

REQUESTS_SESSION = _build_session(max_pool_size=max(DEFAULT_MAX_WORKERS, len(URLS))) if 'URLS' in globals() else _build_session(DEFAULT_MAX_WORKERS)

def fetch_data(url: str, timeout: int = 10, max_attempts: int = 3, session: requests.Session | None = None) -> str:
    sess = session or REQUESTS_SESSION
    for attempt in range(1, max_attempts + 1):
        try:
            modified_url = url
            verify = True

            if attempt == 2:
                verify = False
            elif attempt == 3:
                parsed = urllib.parse.urlparse(url)
                if parsed.scheme == "https":
                    modified_url = parsed._replace(scheme="http").geturl()
                verify = False

            response = sess.get(modified_url, timeout=timeout, verify=verify)
            response.raise_for_status()
            return response.text

        except requests.exceptions.RequestException as exc:
            last_exc = exc
            if attempt < max_attempts:
                continue
            raise last_exc

def save_to_local_file(path, content):
    with open(path, "w", encoding="utf-8") as file:
        file.write(content)
    log(f"üìÅ –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –ª–æ–∫–∞–ª—å–Ω–æ –≤ {path}")

def extract_source_name(url: str) -> str:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø–æ–Ω—è—Ç–Ω–æ–µ –∏–º—è –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –∏–∑ URL"""
    try:
        parsed = urllib.parse.urlparse(url)
        path_parts = parsed.path.split('/')
        if len(path_parts) > 2:
            return f"{path_parts[1]}/{path_parts[2]}"
        return parsed.netloc
    except:
        return "–ò—Å—Ç–æ—á–Ω–∏–∫"

def update_readme_table():
    """–û–±–Ω–æ–≤–ª—è–µ—Ç —Ç–∞–±–ª–∏—Ü—É –≤ README.md —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –≤—Ä–µ–º–µ–Ω–∏ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è"""
    try:
        # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—É—â–∏–π README.md
        try:
            readme_file = REPO.get_contents("README.md")
            old_content = readme_file.decoded_content.decode("utf-8")
        except GithubException as e:
            if e.status == 404:
                log("‚ùå README.md –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏")
                return
            else:
                log(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ README.md: {e}")
                return

        # –†–∞–∑–¥–µ–ª—è–µ–º –≤—Ä–µ–º—è –∏ –¥–∞—Ç—É
        time_part, date_part = offset.split(" | ")
        
        # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é —Ç–∞–±–ª–∏—Ü—É
        table_header = "| ‚Ññ | –§–∞–π–ª | –ò—Å—Ç–æ—á–Ω–∏–∫ | –í—Ä–µ–º—è | –î–∞—Ç–∞ |\n|--|--|--|--|--|"
        table_rows = []
        
        for i, (remote_path, url) in enumerate(zip(REMOTE_PATHS, URLS + [""]), 1):
            filename = f"{i}.txt"
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º —Å—Å—ã–ª–∫—É –Ω–∞ raw-—Ñ–∞–π–ª –≤ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏
            raw_file_url = f"https://github.com/{REPO_NAME}/raw/refs/heads/main/githubmirror/{i}.txt"
            
            if i <= 25:
                source_name = extract_source_name(url)
                source_column = f"[{source_name}]({url})"
            else:
                # –î–ª—è 26-–≥–æ —Ñ–∞–π–ª–∞ —Å–æ–∑–¥–∞–µ–º —Å—Å—ã–ª–∫—É –Ω–∞ —Å–∞–º —Ñ–∞–π–ª —Å —Ç–µ–∫—Å—Ç–æ–º "–û–±—Ö–æ–¥ SNI –±–µ–ª—ã—Ö —Å–ø–∏—Å–∫–æ–≤"
                source_name = "–û–±—Ö–æ–¥ SNI –±–µ–ª—ã—Ö —Å–ø–∏—Å–∫–æ–≤"
                source_column = f"[{source_name}]({raw_file_url})"
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –±—ã–ª –ª–∏ —Ñ–∞–π–ª –æ–±–Ω–æ–≤–ª–µ–Ω –≤ —ç—Ç–æ–º –∑–∞–ø—É—Å–∫–µ
            if i in updated_files:
                update_time = time_part
                update_date = date_part
            else:
                # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –≤—Ä–µ–º—è –∏ –¥–∞—Ç—É –∏–∑ —Å—Ç–∞—Ä–æ–π —Ç–∞–±–ª–∏—Ü—ã
                pattern = rf"\|\s*{i}\s*\|\s*\[`{filename}`\].*?\|.*?\|\s*(.*?)\s*\|\s*(.*?)\s*\|"
                match = re.search(pattern, old_content)
                if match:
                    update_time = match.group(1).strip() if match.group(1).strip() else "–ù–∏–∫–æ–≥–¥–∞"
                    update_date = match.group(2).strip() if match.group(2).strip() else "–ù–∏–∫–æ–≥–¥–∞"
                else:
                    update_time = "–ù–∏–∫–æ–≥–¥–∞"
                    update_date = "–ù–∏–∫–æ–≥–¥–∞"
            
            # –î–ª—è –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤ –¥–µ–ª–∞–µ–º —Å—Å—ã–ª–∫—É –Ω–∞ raw-—Ñ–∞–π–ª –≤ —Å—Ç–æ–ª–±—Ü–µ "–§–∞–π–ª"
            table_rows.append(f"| {i} | [`{filename}`]({raw_file_url}) | {source_column} | {update_time} | {update_date} |")

        new_table = table_header + "\n" + "\n".join(table_rows)

        # –ó–∞–º–µ–Ω—è–µ–º —Ç–∞–±–ª–∏—Ü—É –≤ README.md
        table_pattern = r"\| ‚Ññ \| –§–∞–π–ª \| –ò—Å—Ç–æ—á–Ω–∏–∫ \| –í—Ä–µ–º—è \| –î–∞—Ç–∞ \|[\s\S]*?\|--\|--\|--\|--\|--\|[\s\S]*?(\n\n## |$)"
        new_content = re.sub(table_pattern, new_table + r"\1", old_content)

        if new_content != old_content:
            REPO.update_file(
                path="README.md",
                message="üìù –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã –≤ README.md",
                content=new_content,
                sha=readme_file.sha
            )
            log("üìù –¢–∞–±–ª–∏—Ü–∞ –≤ README.md –æ–±–Ω–æ–≤–ª–µ–Ω–∞")
        else:
            log("üìù –¢–∞–±–ª–∏—Ü–∞ –≤ README.md –Ω–µ —Ç—Ä–µ–±—É–µ—Ç –∏–∑–º–µ–Ω–µ–Ω–∏–π")

    except Exception as e:
        log(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ README.md: {e}")

def upload_to_github(local_path, remote_path):
    if not os.path.exists(local_path):
        log(f"‚ùå –§–∞–π–ª {local_path} –Ω–µ –Ω–∞–π–¥–µ–Ω.")
        return

    repo = REPO

    with open(local_path, "r", encoding="utf-8") as file:
        content = file.read()

    max_retries = 5
    import time

    for attempt in range(1, max_retries + 1):
        try:
            try:
                file_in_repo = repo.get_contents(remote_path)
                current_sha = file_in_repo.sha
            except GithubException as e_get:
                if getattr(e_get, "status", None) == 404:
                    basename = os.path.basename(remote_path)
                    repo.create_file(
                        path=remote_path,
                        message=f"üÜï –ü–µ—Ä–≤—ã–π –∫–æ–º–º–∏—Ç {basename} –ø–æ —á–∞—Å–æ–≤–æ–º—É –ø–æ—è—Å—É –ï–≤—Ä–æ–ø–∞/–ú–æ—Å–∫–≤–∞: {offset}",
                        content=content,
                    )
                    log(f"üÜï –§–∞–π–ª {remote_path} —Å–æ–∑–¥–∞–Ω.")
                    # –î–æ–±–∞–≤–ª—è–µ–º –≤ –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã
                    file_index = int(remote_path.split('/')[1].split('.')[0])
                    with _UPDATED_FILES_LOCK:
                        updated_files.add(file_index)
                    return
                else:
                    msg = e_get.data.get("message", str(e_get))
                    log(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ {remote_path}: {msg}")
                    return

            try:
                remote_content = file_in_repo.decoded_content.decode("utf-8", errors="replace")
                if remote_content == content:
                    log(f"üîÑ –ò–∑–º–µ–Ω–µ–Ω–∏–π –¥–ª—è {remote_path} –Ω–µ—Ç.")
                    return
            except Exception:
                pass

            basename = os.path.basename(remote_path)
            try:
                repo.update_file(
                    path=remote_path,
                    message=f"üöÄ –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ {basename} –ø–æ —á–∞—Å–æ–≤–æ–º—É –ø–æ—è—Å—É –ï–≤—Ä–æ–ø–∞/–ú–æ—Å–∫–≤–∞: {offset}",
                    content=content,
                    sha=current_sha,
                )
                log(f"üöÄ –§–∞–π–ª {remote_path} –æ–±–Ω–æ–≤–ª—ë–Ω –≤ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏.")
                # –î–æ–±–∞–≤–ª—è–µ–º –≤ –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã
                file_index = int(remote_path.split('/')[1].split('.')[0])
                with _UPDATED_FILES_LOCK:
                    updated_files.add(file_index)
                return
            except GithubException as e_upd:
                if getattr(e_upd, "status", None) == 409:
                    if attempt < max_retries:
                        wait_time = 0.5 * (2 ** (attempt - 1))
                        log(f"‚ö†Ô∏è –ö–æ–Ω—Ñ–ª–∏–∫—Ç SHA –¥–ª—è {remote_path}, –ø–æ–ø—ã—Ç–∫–∞ {attempt}/{max_retries}, –∂–¥–µ–º {wait_time} —Å–µ–∫")
                        time.sleep(wait_time)
                        continue
                    else:
                        log(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –æ–±–Ω–æ–≤–∏—Ç—å {remote_path} –ø–æ—Å–ª–µ {max_retries} –ø–æ–ø—ã—Ç–æ–∫")
                        return
                else:
                    msg = e_upd.data.get("message", str(e_upd))
                    log(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {remote_path}: {msg}")
                    return

        except Exception as e_general:
            short_msg = str(e_general)
            if len(short_msg) > 200:
                short_msg = short_msg[:200] + "‚Ä¶"
            log(f"‚ö†Ô∏è –ù–µ–ø—Ä–µ–¥–≤–∏–¥–µ–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ {remote_path}: {short_msg}")
            return

    log(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –æ–±–Ω–æ–≤–ª–∏—Ç—å {remote_path} –ø–æ—Å–ª–µ {max_retries} –ø–æ–ø—ã—Ç–æ–∫")

def download_and_save(idx):
    url = URLS[idx]
    local_path = LOCAL_PATHS[idx]
    try:
        data = fetch_data(url)

        if os.path.exists(local_path):
            try:
                with open(local_path, "r", encoding="utf-8") as f_old:
                    old_data = f_old.read()
                if old_data == data:
                    log(f"üîÑ –ò–∑–º–µ–Ω–µ–Ω–∏–π –¥–ª—è {local_path} –Ω–µ—Ç (–ª–æ–∫–∞–ª—å–Ω–æ). –ü—Ä–æ–ø—É—Å–∫ –∑–∞–≥—Ä—É–∑–∫–∏ –≤ GitHub.")
                    return None
            except Exception:
                pass

        save_to_local_file(local_path, data)
        return local_path, REMOTE_PATHS[idx]
    except Exception as e:
        short_msg = str(e)
        if len(short_msg) > 200:
            short_msg = short_msg[:200] + "‚Ä¶"
        log(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–∏ {url}: {short_msg}")
        return None

def create_filtered_configs():
    """–°–æ–∑–¥–∞–µ—Ç 26-–π —Ñ–∞–π–ª —Å –∫–æ–Ω—Ñ–∏–≥–∞–º–∏, —Å–æ–¥–µ—Ä–∂–∞—â–∏–º–∏ —É–∫–∞–∑–∞–Ω–Ω—ã–µ SNI –¥–æ–º–µ–Ω—ã"""
    sni_domains = [
        "stats.vk-portal.net",
        "sun6-21.userapi.com",
        "sun6-20.userapi.com",
        "avatars.mds.yandex.net",
        "queuev4.vk.com",
        "sun6-22.userapi.com",
        "sync.browser.yandex.net",
        "top-fwz1.mail.ru",
        "ad.mail.ru",
        "eh.vk.com",
        "akashi.vk-portal.net",
        "sun9-38.userapi.com",
        "st.ozone.ru",
        "ir.ozone.ru",
        "vt-1.ozone.ru",
        "io.ozone.ru",
        "ozone.ru",
        "xapi.ozon.ru",
        "top-fwz1.mail.ru",
        "strm-rad-23.strm.yandex.net",
        "online.sberbank.ru",
        "esa-res.online.sberbank.ru",
        "egress.yandex.net",
        "st.okcdn.ru",
        "rs.mail.ru",
        "counter.yadro.ru",
        "742231.ms.ok.ru",
        "splitter.wb.ru",
        "a.wb.ru",
        "user-geo-data.wildberries.ru",
        "banners-website.wildberries.ru",
        "chat-prod.wildberries.ru",
        "servicepipe.ru",
        "alfabank.ru",
        "statad.ru",
        "alfabank.servicecdn.ru",
        "alfabank.st",
        "ad.adriver.ru",
        "privacy-cs.mail.ru",
        "imgproxy.cdn-tinkoff.ru",
        "mddc.tinkoff.ru",
        "le.tbank.ru",
        "hrc.tbank.ru",
        "id.tbank.ru",
        "rap.skcrtxr.com",
        "eye.targetads.io",
        "px.adhigh.net",
        "top-fwz1.mail.ru",
        "nspk.ru",
        "sba.yandex.net",
        "identitystatic.mts.ru",
        "tag.a.mts.ru",
        "login.mts.ru",
        "serving.a.mts.ru",
        "cm.a.mts.ru",
        "login.vk.com",
        "api.a.mts.ru",
        "mtscdn.ru",
        "d5de4k0ri8jba7ucdbt6.apigw.yandexcloud.net",
        "moscow.megafon.ru",
        "api.mindbox.ru",
        "web-static.mindbox.ru",
        "storage.yandexcloud.net",
        "personalization-web-stable.mindbox.ru",
        "www.t2.ru",
        "beeline.api.flocktory.com",
        "static.beeline.ru",
        "moskva.beeline.ru",
        "wcm.weborama-tech.ru",
        "1013a--ma--8935--cp199.stbid.ru",
        "msk.t2.ru",
        "s3.t2.ru",
        "get4click.ru",
        "dzen.ru",
        "yastatic.net",
        "csp.yandex.net",
        "sntr.avito.ru",
        "yabro-wbplugin.edadeal.yandex.ru",
        "cdn.uxfeedback.ru",
        "goya.rutube.ru",
        "api.expf.ru",
        "fb-cdn.premier.one",
        "www.kinopoisk.ru",
        "widgets.kinopoisk.ru",
        "payment-widget.plus.kinopoisk.ru",
        "api.events.plus.yandex.net",
        "tns-counter.ru",
        "speller.yandex.net",
        "widgets.cbonds.ru",
        "www.magnit.com",
        "magnit-ru.injector.3ebra.net",
        "jsons.injector.3ebra.net",
        "2gis.ru",
        "d-assets.2gis.ru",
        "s1.bss.2gis.com",
        "www.tbank.ru",
        "strm-spbmiran-08.strm.yandex.net",
        "id.tbank.ru",
        "tmsg.tbank.ru",
        "vk.com",
        "www.wildberries.ru",
        "www.ozon.ru",
        "ok.ru",
        "yandex.ru",
        "www.unicreditbank.ru",
        "www.gazprombank.ru",
        "cdn.gpb.ru",
        "mkb.ru",
        "www.open.ru",
        "cobrowsing.tbank.ru",
        "cdn.rosbank.ru",
        "www.psbank.ru",
        "www.raiffeisen.ru",
        "www.rzd.ru",
        "st.gismeteo.st",
        "stat-api.gismeteo.net",
        "c.dns-shop.ru",
        "restapi.dns-shop.ru",
        "www.pochta.ru",
        "passport.pochta.ru",
        "chat-ct.pochta.ru",
        "www.x5.ru",
        "www.ivi.ru",
        "api2.ivi.ru",
        "hh.ru",
        "i.hh.ru",
        "hhcdn.ru",
        "sentry.hh.ru",
        "cpa.hh.ru",
        "www.kp.ru",
        "cdnn21.img.ria.ru",
        "lenta.ru",
        "sync.rambler.ru",
        "s.rbk.ru",
        "www.rbc.ru",
        "target.smi2.net",
        "hb-bidder.skcrtxr.com",
        "strm-spbmiran-07.strm.yandex.net",
        "pikabu.ru",
        "www.tutu.ru",
        "cdn1.tu-tu.ru",
        "api.apteka.ru",
        "static.apteka.ru",
        "images.apteka.ru",
        "scitylana.apteka.ru",
        "www.drom.ru",
        "c.rdrom.ru",
        "www.farpost.ru",
        "s11.auto.drom.ru",
        "i.rdrom.ru",
        "yummy.drom.ru",
        "www.drive2.ru",
        "lemanapro.ru"
    ]
    
    all_configs = []

    # –ß–∏—Ç–∞–µ–º –≤—Å–µ 25 —Ñ–∞–π–ª–æ–≤ –∏ —Å–æ–±–∏—Ä–∞–µ–º –∫–æ–Ω—Ñ–∏–≥–∏, —Å–æ–¥–µ—Ä–∂–∞—â–∏–µ —É–∫–∞–∑–∞–Ω–Ω—ã–µ –¥–æ–º–µ–Ω—ã
    for i in range(1, 26):
        local_path = f"githubmirror/{i}.txt"
        if os.path.exists(local_path):
            try:
                with open(local_path, "r", encoding="utf-8") as file:
                    for line in file:
                        line = line.strip()
                        if any(domain in line for domain in sni_domains):
                            all_configs.append(line)
            except Exception as e:
                log(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ —Ñ–∞–π–ª–∞ {local_path}: {e}")

    def _extract_host_port(line: str):
        """–ü—Ä–æ–±—É–µ—Ç –∏–∑–≤–ª–µ—á—å host –∏ port –∏–∑ —Å—Ç—Ä–æ–∫–∏ –∫–æ–Ω—Ñ–∏–≥–∞.
        –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ñ–æ—Ä–º–∞—Ç–æ–≤: vmess://<base64-json>, –æ–±—ã—á–Ω—ã–µ URI —Å —Å—Ö–µ–º–æ–π,
        –∞ —Ç–∞–∫–∂–µ –ø—Ä–æ—Å—Ç—ã–µ –≤—Ö–æ–∂–¥–µ–Ω–∏—è host:port –∏–ª–∏ ip:port —á–µ—Ä–µ–∑ regex.
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ—Ä—Ç–µ–∂ (host, port) –∏–ª–∏ None.
        """
        if not line:
            return None

        # vmess://<base64>
        try:
            if line.lower().startswith("vmess://"):
                payload = line[len("vmess://"):]
                # –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–µ–º –ø–∞–¥–¥–∏–Ω–≥ –∏ –¥–µ–∫–æ–¥–∏—Ä—É–µ–º
                try:
                    payload_bytes = base64.b64decode(payload + '=' * (-len(payload) % 4))
                    decoded = payload_bytes.decode('utf-8', errors='ignore')
                    j = json.loads(decoded)
                    host = j.get('add') or j.get('host') or j.get('ip')
                    port = j.get('port')
                    if host and port:
                        return host, str(port)
                except Exception:
                    pass
        except Exception:
            pass

        # –ü–æ–ø—ã—Ç–∫–∞ —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å –∫–∞–∫ URI (trojan://, vless://, http:// –∏ —Ç.–¥.)
        try:
            parsed = urllib.parse.urlparse(line if '://' in line else '//' + line)
            if parsed.hostname and parsed.port:
                return parsed.hostname, str(parsed.port)
        except Exception:
            pass

        # –ò—â–µ–º —è–≤–Ω–æ–µ –≤—Ö–æ–∂–¥–µ–Ω–∏–µ host:port –∏–ª–∏ ip:port
        m = re.search(r'(?P<host>(?:\d{1,3}\.){3}\d{1,3}|[A-Za-z0-9\-_.]+):(?P<port>\d{1,5})', line)
        if m:
            return m.group('host'), m.group('port')

        return None

    # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã: —Å–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ–ª–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Å—Ç—Ä–æ–∫–∏, –∑–∞—Ç–µ–º
    # —Å—á–∏—Ç–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç–æ–º –∫–æ–Ω—Ñ–∏–≥ —Å —Ç–µ–º –∂–µ host:port (ip:port)
    seen_full = set()
    seen_hostport = set()
    unique_configs = []

    for cfg in all_configs:
        c = cfg.strip()
        if not c:
            continue

        if c in seen_full:
            continue
        seen_full.add(c)

        hostport = _extract_host_port(c)
        if hostport:
            key = f"{hostport[0].lower()}:{hostport[1]}"
            if key in seen_hostport:
                # —É–∂–µ –µ—Å—Ç—å —Å–µ—Ä–≤–µ—Ä —Å —Ç–∞–∫–∏–º –∂–µ host:port ‚Äî —Å—á–∏—Ç–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç–æ–º
                continue
            seen_hostport.add(key)

        unique_configs.append(c)

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ 26-–π —Ñ–∞–π–ª
    local_path_26 = "githubmirror/26.txt"
    try:
        with open(local_path_26, "w", encoding="utf-8") as file:
            for config in unique_configs:
                file.write(config + "\n")
        log(f"üìÅ –°–æ–∑–¥–∞–Ω —Ñ–∞–π–ª {local_path_26} —Å {len(unique_configs)} –∫–æ–Ω—Ñ–∏–≥–∞–º–∏, —Å–æ–¥–µ—Ä–∂–∞—â–∏–º–∏ —É–∫–∞–∑–∞–Ω–Ω—ã–µ SNI –¥–æ–º–µ–Ω—ã")
    except Exception as e:
        log(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ {local_path_26}: {e}")

    return local_path_26

def main(dry_run: bool = False):
    max_workers_download = min(DEFAULT_MAX_WORKERS, max(1, len(URLS)))
    max_workers_upload = max(2, min(6, len(URLS)))

    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers_download) as download_pool, \
         concurrent.futures.ThreadPoolExecutor(max_workers=max_workers_upload) as upload_pool:

        download_futures = [download_pool.submit(download_and_save, i) for i in range(len(URLS))]
        upload_futures: list[concurrent.futures.Future] = []

        for future in concurrent.futures.as_completed(download_futures):
            result = future.result()
            if result:
                local_path, remote_path = result
                if dry_run:
                    log(f"‚ÑπÔ∏è Dry-run: –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –∑–∞–≥—Ä—É–∑–∫—É {remote_path} (–ª–æ–∫–∞–ª—å–Ω—ã–π –ø—É—Ç—å {local_path})")
                else:
                    upload_futures.append(upload_pool.submit(upload_to_github, local_path, remote_path))

        for uf in concurrent.futures.as_completed(upload_futures):
            _ = uf.result()

    # –°–æ–∑–¥–∞–µ–º 26-–π —Ñ–∞–π–ª —Å –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –∫–æ–Ω—Ñ–∏–≥–∞–º–∏
    local_path_26 = create_filtered_configs()
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º 26-–π —Ñ–∞–π–ª –≤ GitHub
    if not dry_run:
        upload_to_github(local_path_26, "githubmirror/26.txt")

    # –û–±–Ω–æ–≤–ª—è–µ–º —Ç–∞–±–ª–∏—Ü—É –≤ README.md –ø–æ—Å–ª–µ –≤—Å–µ—Ö –∑–∞–≥—Ä—É–∑–æ–∫
    if not dry_run and updated_files:
        update_readme_table()

    # –í—ã–≤–æ–¥ –ª–æ–≥–æ–≤
    ordered_keys = sorted(k for k in LOGS_BY_FILE.keys() if k != 0)
    output_lines: list[str] = []

    for k in ordered_keys:
        output_lines.append(f"----- {k}.txt -----")
        output_lines.extend(LOGS_BY_FILE[k])

    if LOGS_BY_FILE.get(0):
        output_lines.append("----- –û–±—â–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è -----")
        output_lines.extend(LOGS_BY_FILE[0])

    print("\n".join(output_lines))

if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="–°–∫–∞—á–∏–≤–∞–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥–æ–≤ –∏ –∑–∞–≥—Ä—É–∑–∫–∞ –≤ GitHub")
    parser.add_argument("--dry-run", action="store_true", help="–¢–æ–ª—å–∫–æ —Å–∫–∞—á–∏–≤–∞—Ç—å –∏ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å –ª–æ–∫–∞–ª—å–Ω–æ, –Ω–µ –∑–∞–≥—Ä—É–∂–∞—Ç—å –≤ GitHub")
    args = parser.parse_args()

    main(dry_run=args.dry_run)